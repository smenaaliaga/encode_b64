{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WzS64v7MGtO5"
      },
      "source": [
        "# Mini Proyecto 3\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izvUgqlHGryC"
      },
      "source": [
        "## Instalación de ambiente\n",
        "Debe subir el archivo \"miniproyecto_installer.py\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5bBS0t22Ib8n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2e2656c-1628-4de3-d71c-c532cd96d734"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P7hxPTPGIkZW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b9c30cd-4682-4aba-9c33-9ded5bd783a4"
      },
      "source": [
        "exec(open('/content/drive/MyDrive/BigDataSw/miniproyecto3_installer.py').read())"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Active services:\n",
            "4080 NodeManager\n",
            "3777 ResourceManager\n",
            "4229 JobHistoryServer\n",
            "3847 NameNode\n",
            "3927 DataNode\n",
            "4297 Jps\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0SIWJwzksPj"
      },
      "source": [
        "## Actividad 1\n",
        "Visualización de datos en MySQL: *Hate Speech and Offensive Language Dataset*\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8waOaU_gaQSn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6670c625-4e0b-45a2-b7ba-5e4f2ad7e77c"
      },
      "source": [
        "!mysql # completar"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ERROR 1045 (28000): Access denied for user 'root'@'localhost' (using password: NO)\n",
            "\u0007"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-r_Pmt_JUKG"
      },
      "source": [
        "## Actividad 2\n",
        "Inserción de datos con Sqoop. El password de MySQL es \"password\" (sin comillas)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l2YrBx52JVZO"
      },
      "source": [
        "!sqoop # completar"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56HzogOHYaor"
      },
      "source": [
        "## Actividad 3\n",
        "Visualización de datos con Hive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CMeA6eGljJeD"
      },
      "source": [
        "!hive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-UxCQ8OlJai"
      },
      "source": [
        "## Actividad 4\n",
        "Lectura de datos con Spark SQL y separación train/test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fqf85DDY5LX4"
      },
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.enableHiveSupport().appName(\"MP-3\").getOrCreate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EeI5IS7A5q0V"
      },
      "source": [
        "df_hate = #completar\n",
        "df_hate = df_hate.withColumn('label', df_hate['class']+1)\n",
        "\n",
        "training, testing = # completar"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iccj4dmwZEQV"
      },
      "source": [
        "## Actividad 5\n",
        "Preprocesamiento de dataset de entrenamiento, con funciones de _tokenización_ y remoción de _stopwords_"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6YPEEGH0NP4H"
      },
      "source": [
        "from pyspark.ml.feature import RegexTokenizer\n",
        "from pyspark.ml.feature import StopWordsRemover\n",
        "\n",
        "tokenizer = RegexTokenizer(pattern=\"\\\\W+\", \"<<COMPLETAR>>\" )\n",
        "\n",
        "stopWords=StopWordsRemover.loadDefaultStopWords('english')\n",
        "stopWordsRemover = StopWordsRemover(stopWords = stopWords, \"<<COMPLETAR>>\" )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_x93bhsVBlFD"
      },
      "source": [
        "# tokenizer\n",
        "training_words = \"<<COMPLETAR>>\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvSM_jUYMJ4-"
      },
      "source": [
        "# mostrar las columnas \"words\" y \"tweet\" de training_words\n",
        "training_words.\"<<COMPLETAR>>\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFO9XubcMNbh"
      },
      "source": [
        "# stopWordsRemover\n",
        "training_clean = \"<<COMPLETAR>>\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9sbeCI6tN0xn"
      },
      "source": [
        "from pyspark.sql.functions import size\n",
        "\n",
        "# Se usan finalmente aquellas frases con más de 3 palabras que no sean stopwords\n",
        "train_words = training_clean.filter(size(training_clean['clean_words']) > 3)\n",
        "train_words.show(n=5, truncate=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-CAP88mbdBz"
      },
      "source": [
        "## Actividad 6\n",
        "Entrenamiento de modelo Word2Vec y transformación a features numéricos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VOI9aA5TN7DJ"
      },
      "source": [
        "from pyspark.ml.feature import Word2Vec\n",
        "\n",
        "model_w2v = Word2Vec(\"<<COMPLETAR>>\").fit(train_words)\n",
        "train_features = model_w2v.transform(train_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJwAlNXtWkFQ"
      },
      "source": [
        "# mostrar Dataframe final que se usará para entrenamiento\n",
        "train_features.\"<<COMPLETAR>>\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cWlKGWqZdCc1"
      },
      "source": [
        "## Actividad 7\n",
        "Entrenamiento de modelo clasificador RandomForestClassifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n1lkC7O6dGbc"
      },
      "source": [
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "\n",
        "algorithm = RandomForestClassifier(\"<<COMPLETAR>>\")\n",
        "model = \"<<COMPLETAR>>\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27LL3o__eDpY"
      },
      "source": [
        "# obtener Dataframes de testing\n",
        "\n",
        "testing_words = \"<<COMPLETAR>>\"\n",
        "testing_clean = \"<<COMPLETAR>>\"\n",
        "test_words = testing_clean.filter(size(testing_clean['clean_words']) > 3)\n",
        "test_features = \"<<COMPLETAR>>\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qFv6pDDLew-5"
      },
      "source": [
        "# obtener predicciones\n",
        "\n",
        "predictions = \"<<COMPLETAR>>\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TWgoIbfwe8JF"
      },
      "source": [
        "# mostrar label, predicciones, y el tweet original\n",
        "\n",
        "predictions.\"<<COMPLETAR>>\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4XHHp7Lpfa7n"
      },
      "source": [
        "# evaluación con métrica de accuracy\n",
        "\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "evaluator = MulticlassClassificationEvaluator(predictionCol='prediction', labelCol='label', metricName='accuracy')\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "print(\"Test accuracy: \", \"%2.1f%%\" % (accuracy*100,))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cbEXnc5uHi58"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}